---
title: "Imputation"
author: "Jae Yeon Kim"
output:
  html_document: 
    toc: true
    theme: united
---

## 0. Setup 

I tweaked the global option of the R Markdown to resize figures produced by ggplot2.

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width = 12, fig.height = 12, 
                      echo = FALSE, warning = FALSE, message = FALSE) # global setting for enlarging image size
```

```{r}

# Clean up the environment

# rm(list = ls())

# Import libraries (adapted from this link: https://stackoverflow.com/questions/4090169/elegant-way-to-check-for-missing-packages-and-install-them)

if (!require("pacman")) install.packages("pacman")
pacman::p_load(
        mice, # for imputation
        miceadds, # additional functions for mice
        tidyverse, # for the tidyverse framework 
        naniar, # for missing values 
        MissMech, # testing patterns among missing values 
        BaylorEdPsych, # testing patterns among missing values
        ggthemes, # for fancy ggplot themes
        here, # for self-contained projects
        ggpubr # for pub-ready themes 
)

devtools::install_github("jaeyk/makereproducible",
        dependencies = TRUE)
        
library(makereproducible)

# for publication-friendly theme 
theme_set(theme_pubr())

```

## 1. Importing files 

Importing the file that we created in `01_data_cleaning.Rmd`.

```{r}

# Import 

cleaned <- read.csv(make_here("/home/jae/analyzing-racial-lived-experience/processed_data/cleaned.csv"))[,-1] # ignore the first column (=row numbers)

```

## 2. Examining missing responses

- Listwise deletion (`na.rm = TRUE`) works only if these observations are missing completely at random (MCAR). This is a strong assumption, as it happens very rarely. 

### 2.1. Exploratory analysis (data visualization)

- The `nanair package` provides many useful functions for inspecting missing data values. I used the `miss_var_summary() function` from this package to inspect the missing pattern and visualized it using ggplot2. 
- `q10_2b` education is seriously missing observations (>75%) for whites, blacks, and multiracial Americans but not for Asian American Pacific Islanders (AAPI) and Latinos. 
- More or less 50% missingness exist among everyday challenged related variables (starts with `q5_7`) across all racial groups.

```{r}

# Visualize the percentaeg of missing values by variables and subgroups 

cleaned %>%
    group_by(race) %>% # group by race
    miss_var_summary() %>% # summary 
    filter(n_miss != 0) %>% # show variables that include missing values
    ggplot(aes(x = reorder(variable, pct_miss), y = pct_miss)) +
        geom_col() +
        facet_wrap(~race) + # facet by race 
        coord_flip() # flip axes 

ggsave(make_here("/home/jae/analyzing-racial-lived-experience/outputs/missing_rate.png"), height = 8)
```

### 2.2. Little's Test 

Now, let's formally test whether the data misses values by random. Here, I used [Roderik J. A. Little's global test for MCAR](https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478722?casa_token=xXGRTDzkeB4AAAAA:Ji89bNk4Sh9VXEoEJrEl89Iv3JrH2s0onf4gfOz4TEb1rB9DAkVJUyVc7eU9LDYVuhIaxkBZ9uQS). The null hypothesis is the data is MCAR. The test result shows that the chance for MCAR is extreme (low p-value). Therefore, listwise deletion is discouraged as an option. 

```{r eval=FALSE, include=FALSE}

little_test_result <- cleaned %>%
    BaylorEdPsych::LittleMCAR()

little_test_result$p.value

```


## 3. Multiple imputation 

In this case, I took a multiple imputation approach. Basically, I recovered (imputed) missing values based on the observed data. I did so by making simulations of these imputed values in order to garner some measures of their uncertainty.

### 3.1. Imputing data 

In this project, I used the `mice package` developed by [Stef van Buuren](https://stefvanbuuren.name/).  The following code shows how the imputation model is set up. I comment on each argument to make it explicit how the model is set up. For instance, the `m argument` refers to the number of imputations, and I set it to `5`.

```{r}

imp <- mice(cleaned,
    seed = 1234, # for reproducibility
    m = 5, # the number of imputations
    maxit = 10, # the max numbe of iterations 
    method = "pmm", # predictive mean method
    print = FALSE) 

```

### 3.2. Evaluating imputations

#### 3.2.1. Data visualization 

The goal of imputation is to create imputed values that are as close as possible to observed values. I used the `densityplot() function` from the `mice package` to display the kernel density estimates (KDE) for the marginal distribution of the imputed (red) and the observed (blue) values. Kernel density estimation is calculated by weighting the distance of the data points. The plot shows that the distributions of the imputed and observed values are quite close, especially among items on a Likert scale. 


```{r}

densityplot(imp, layout = c(1, 3))

```

#### 3.2.2. Rubin's rule test 

Another critical test is deciding whether selecting one imputed piece of data over another is consequential. The `complete() function` pools the imputed data, but a question remains about which one should be selected. In this project, I created five imputed datasets. I built a simple regression model that examines associations between survey items related to the everyday challenge component of racial experience. The `pool() function` from the `mice package` is Donald Rubin’s rule test. The test “averages the estimates of the complete model” and “computes the total variance over the repeated analysis” (for more information, see [this function documentation](https://rdrr.io/cran/mice/man/pool.html)). The summary test result shows that the p-values for the regression coefficients are extremely small. Selecting one model over the other makes little difference for the model fit (=little within imputation variance). Rubin’s rule test assumes that the distribution of the data follows a normal distribution. I checked this assumption using the Shapiro-Wilk test.   

```{r}

# Select test variables from the original data 

test_vars <- imp$data %>%
  dplyr::select(q5_7_a, q5_7_b, q5_7_c, q5_7_d, q5_7_e, q5_7_f, q5_7_g, q5_7_h, q5_7_i, q5_7_j, q5_7_k, q5_7_l)

# Shapiro test 

apply(test_vars, 2, shapiro.test)

# Fitting the data to the OLS regression model

fit <- with(data = imp, expr = lm(q5_7_a ~ # visa delay 
    q5_7_b + # school quality 
    q5_7_c + # school bullied 
    q5_7_d + # college affordability 
    q5_7_e + # elderly care 
    q5_7_f + # medical care
    q5_7_g + # rent affordability 
    q5_7_h + # college debt
    q5_7_i + # medical debt 
    q5_7_j + # card debt
    q5_7_k + # childcare 
    q5_7_l)) # little saving  

```

```{r}

# Pooling them together

comb_fit <- pool(fit)

summary(comb_fit)

```

### 3.4. Pooling results

```{r}
# Pooling results 

imputed <- mice::complete(imp) # the first imputed data

# Check 

paste("The number of missing values in the data after multiple imputation is", sum(is.na(imputed)))

```

## 4. Save the imputed data

```{r}

write.csv(imputed, make_here("/home/jae/analyzing-racial-lived-experience/processed_data/imputed.csv"))

```
